### 切片集群架构图

[![ywSokV.md.jpg](https://z3.ax1x.com/2021/02/10/ywSokV.md.jpg)](https://imgtu.com/i/ywSokV)

#### redis cluster

- Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384
  个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。

- 键的映射过程

（1）首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值

（2）再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽

- 哈希槽映射过程

（1）我们在部署 Redis Cluster 方案时，可以使用 **cluster create** 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上； 如果有N个实例，那么每个实例上的槽个数为16384/N

(2) 使用 **cluster meet** 命令手动建立实例间的连接，形成集群，再使用 **cluster addslots** 命令，指定每个实例上的哈希槽个数

````
在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。
````

### 客户端如何定位数据

- 在定位键值对数据时，它所处的哈希槽是可以通过计算得到的，这个计算可以在客户端发送请求时来执行

- Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，**每个实例就有所有哈希槽的映射关系了**

- 客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了

#### 重定向机制

- Redis Cluster 方案提供了一种重定向机制，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

- 当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回**MOVED** 命令响应结果，这个结果中就包含了新实例的访问地址

- ASK 命令表示两层含义：第一，表明 Slot 数据还在迁移中；第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。和 MOVED 命令不同，ASK
  命令并不会更新客户端缓存的哈希槽分配信

[![ywpsBR.md.jpg](https://z3.ax1x.com/2021/02/10/ywpsBR.md.jpg)](https://imgtu.com/i/ywpsBR)

### Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：

1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。

2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），
这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。

3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。

4、而在中间增加一层哈希槽，可以把**数据和实例节点解耦**，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理

### 请求路由

- 一般都是采用哈希槽的映射关系表找到指定节点，然后在这个节点上操作的方案。

- Redis Cluster在每个节点记录完整的映射关系(便于纠正客户端的错误路由请求)，同时也发给客户端让客户端缓存一份，便于客户端直接找到指定节点，客户端与服务端配合完成数据的路由，这需要业务在使用Redis Cluster时，必须升级为集群版的SDK才支持客户端和服务端的协议交互。

- 其他Redis集群化方案例如**Twemproxy、Codis**都是中心化模式（增加Proxy层），客户端通过Proxy对整个集群进行操作，Proxy后面可以挂N多个Redis实例，Proxy层维护了路由的转发逻辑。操作Proxy就像是操作一个普通Redis一样，客户端也不需要更换SDK，而Redis
Cluster是把这些路由逻辑做在了SDK中。当然，增加一层Proxy也会带来一定的性能损耗。

### 数据迁移

#### 当集群节点不足以支撑业务需求时，就需要扩容节点，扩容就意味着节点之间的数据需要做迁移，而迁移过程中是否会影响到业务，这也是判定一个集群方案是否成熟的标准。

- Twemproxy不支持在线扩容，它只解决了请求路由的问题，扩容时需要停机做数据重新分配。

- Redis Cluster和Codis都做到了在线扩容（不影响业务或对业务的影响非常小），重点就是在数据迁移过程中，客户端对于正在迁移的key进行操作时，集群如何处理？还要保证响应正确的结果？

- Redis Cluster和Codis都需要服务端和客户端/Proxy层互相配合，迁移过程中，服务端针对正在迁移的key，需要让客户端或Proxy去新节点访问（重定向），这个过程就是为了保证业务在访问这些key时依旧不受影响，而且可以得到正确的结果。由于重定向的存在，所以这个期间的访问延迟会变大。等迁移完成之后，Redis
Cluster每个节点会更新路由映射表，同时也会让客户端感知到，更新客户端缓存。
  
- Codis会在Proxy层更新路由表，客户端在整个过程中无感知。

- 除了访问正确的节点之外，数据迁移过程中还需要解决异常情况（迁移超时、迁移失败）、性能问题（如何让数据迁移更快、bigkey如何处理），这个过程中的细节也很多。

- Redis Cluster的数据迁移是同步的，迁移一个key会同时阻塞源节点和目标节点，迁移过程中会有性能问题。而Codis提供了异步迁移数据的方案，迁移速度更快，对性能影响最小，当然，实现方案也比较复杂。

### 16384

因为集群中每个节点需要交换各自的路由信息，也就是槽位信息，Redis也需要考虑交换的成本，占用的网络资源。

过多的槽位在交换信息时也会变得很重，所以Redis作者在设计时做了权衡，尽量使用少的内存完成信息交换，在设计内存存储时定的16384，作者预估一个集群不会超过1000个实例。

### 切片的主从配置

切片集群中每个切片可以配置从库，也可以不配置。不过一般生产环境中还是建议对每个切片做主从配置。 可以使用**cluster replicate**命令进行配置。