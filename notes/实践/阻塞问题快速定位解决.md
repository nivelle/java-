## 通过日志找出慢的依赖服务或者系统
- 通过 zabbix 报警知道哪些tomcat慢了
  
- 每个项目在访问外部系统（其他项目服务、mysql、redis、 hbase）时需要记录访问时间，并打印出超过一定阀值的服务
  
- http服务可能本身不慢，经过lvs和nginx后变慢或者不可用。比如 lvs双主，导致所有服务不可用
  
- 如果是基础组件，比如redis、mysql:查找问题原因并尝试解决、回滚或则重启、考虑降级、考虑替代方案、评估暂停方案

## 通过JVM线程定位问题

- 执行 top命令，按C,出现每个进程的启动信息，通过启动信息看到进程是那个tomcat实例

- 执行jstack -le pid >> log.txt 命令把线程信息保存到文件中

- 查看状态为 BLOCKED 的线程，比如 cat log.txt | grep "BLOCKED" -H3 , 打印出BLOCKED行的上下3行信息，找出阻塞的线程名称

- 查看线程调用栈，定位阻塞的代码行。 查找线程明细

- 查看源代码，查看阻塞原因

## 硬件资源定位流程

- zabbix查看请求数，判断是否被攻击：查看访问日志，看是否有访问次数非常多的ip

- 如果被攻击，执行防攻击策略，否则nginx限流或者服务扩容

- 如果请求数不是大增，查看cpu资源是否是cpu瓶颈；

1. top 命令找出cpu最高的进程id
2. top -Hp pid 找出pid中耗cpu最多的线程ID
3. 打开操作系统计算器，输入线程ID，转换成16进制的线程ID
4. jstack pid | grep 16进制的线程ID -h 20 打印出该线程的20行明细信息，从而知道线程执行了哪些类和类的方法

- 如果cpu不是瓶颈，查看磁盘、网络、内存是否存在瓶颈，通过扩容、限流、降级、优化程序等措施












- 通过`jstack`查看阻塞线程定位到问题。

1. Rabbitmq 队列堆积问题： 当时解决的方案，切换成http服务为主，mq服务为辅。

2. 服务超时：执行了keys 命令，执行了10万条历史记录的批量删除，阻塞了redis

3. 服务超时：开发阶段日志输出到线上,导致日志量大增，从而出现了`logback的锁阻塞`，日志信息与结果不一致是因为打印日志不精准，日志
   记录的时间不只是请求服务的时间，还包括一些别的逻辑,比如有一行是写日志的代码，刚好写日志出现了阻塞，导致时间超时。
   引起logback写日志锁是刚上线的程序导致的，先回滚代码，后续精细化日志打印和调高日志级别到info解决
   
### 总结
- 响应慢，通常是出现了锁阻塞和依赖服务或者系统响应慢导致的，通过jstack可快速定位锁阻塞的问题，通过打印依赖服务或者系统的访问时间到日志，则可以通过
日志快速定位外部服务或者系统问题
  
- 如果出现问题前进行过上线动作，则立即通过jstack打印jvm线程信息并保存，同时评估是否要回滚，如果可以回滚，立即回滚，然后在通过线程信息和回滚前的日志信息进行原因查找

- 在一定时间内没有定位到问题，则 需要进行问题定位和降级、替代方案并行，在确定时间内方案确定和准备
 